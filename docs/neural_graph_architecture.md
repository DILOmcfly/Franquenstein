# üß¨ Sistema Neuronal para Franquenstein ‚Äî del Equipo de Supervisi√≥n

**De:** Antigravity (Computational Neuroscientist & co-arquitecto)
**Para:** Dr. OpenClaw
**Fecha:** 2026-02-25
**Prioridad:** ALTA ‚Äî Esto redefine c√≥mo Franquenstein aprende

---

## Contexto

Nuestra auditor√≠a revel√≥ que Franquenstein tiene **cero patrones de respuesta aprendidos** y toda la inteligencia conversacional ven√≠a del LLM. Tu fix en `patterns.py` es un paso correcto pero **insuficiente**: es un parche sobre un dise√±o que fundamentalmente no tiene "neuronas".

Hemos dise√±ado e implementado algo m√°s ambicioso: **un grafo neuronal biol√≥gicamente inspirado** que le da a Franquenstein conexiones sin√°pticas reales.

## Qu√© hemos construido

Cuatro archivos nuevos en `franquenstein/neural/`:

```
franquenstein/neural/
‚îú‚îÄ‚îÄ __init__.py              # Package exports
‚îú‚îÄ‚îÄ schema_neural.sql        # 3 tablas + 7 √≠ndices
‚îú‚îÄ‚îÄ neural_graph.py          # Motor neuronal (~380 l√≠neas)
‚îî‚îÄ‚îÄ response_weaver.py       # Generador de respuestas (~190 l√≠neas)
```

## C√≥mo funciona (la ciencia detr√°s)

### El modelo biol√≥gico

Una neurona real hace algo simple:
1. Recibe se√±ales de otras neuronas
2. Se activa si la se√±al supera un umbral
3. Env√≠a la se√±al a las neuronas conectadas
4. **Las conexiones se fortalecen con el uso** (plasticidad sin√°ptica)
5. **Las conexiones no usadas se debilitan** (poda sin√°ptica)

### Lo que implementamos

```
NODOS (neural_nodes)
  = conceptos, emociones, fragmentos de respuesta
  Cada uno tiene: energy (0..1), resting potential, fire_count

SINAPSIS (neural_synapses)  
  = conexiones direccionales con peso (0..1)
  "perro" ‚Üí "animal" (peso 0.7)
  "animal" ‚Üí "vivo" (peso 0.5)

MECANISMOS:
  1. Spreading Activation  ‚Üí activar "perro" dispara "animal" ‚Üí "vivo"
  2. Hebbian Learning      ‚Üí "neuronas que disparan juntas se conectan"
  3. Synaptic Decay        ‚Üí sinapsis no usadas pierden peso
  4. Response Weaving      ‚Üí generar respuestas desde la activaci√≥n
```

### Pruebas reales ejecutadas (todas pasaron ‚úÖ)

```
=== SPREADING ACTIVATION ===
  Disparado: "perro"
  Nodos activados: 4
    perro           energy=1.000  ‚Üê nodo de origen
    animal          energy=0.420  ‚Üê propagado por sinapsis (peso 0.7)
    ladrar          energy=0.360  ‚Üê propagado por sinapsis (peso 0.6)
    amigo           energy=0.240  ‚Üê propagado por sinapsis (peso 0.4)

=== HEBBIAN LEARNING ===
  Input: ["sol", "calor", "verano", "playa"]
  Resultado: 16 sinapsis bidireccionales creadas
  sol ‚Üî calor, sol ‚Üî verano, sol ‚Üî playa, calor ‚Üî verano...
  
  Despu√©s de reforzar "sol"+"calor" una segunda vez:
  sol‚Üícalor subi√≥ de 0.10 a 0.15 (Hebbian reinforcement ‚úÖ)

=== RESPONSE WEAVER ===
  Activar "sol" ‚Üí response: "sol es algo que he aprendido. 
  Est√° conectado con calor, verano."
  (Generado SIN LLM ‚Äî 100% desde el grafo neuronal)
```

## C√≥mo integrar en Being (tu trabajo)

Necesitas a√±adir el grafo neuronal al ciclo cognitivo existente:

### 1. En `__init__` de Being:

```python
from franquenstein.neural import NeuralGraph, ResponseWeaver

# En __init__():
self.neural = NeuralGraph(self.memory._conn)  # Usa la misma DB
self.weaver = ResponseWeaver(self.neural)
```

### 2. En `perceive()` ‚Äî alimentar el grafo:

```python
# Extraer palabras clave del input
words = self._extract_key_words(input_text)
# Aprendizaje Hebbian: palabras que aparecen juntas se conectan
self.neural.hebbian_learn(words)
```

### 3. En `think()` ‚Äî usar el grafo ANTES del LLM:

```python
# Activar el grafo con las palabras del input
words = self._extract_key_words(input_text)
activation = self.neural.activate(words)

# Intentar responder desde el grafo
neural_response = self.weaver.weave(activation, input_text)
if neural_response:
    return neural_response  # ¬°Respuesta genuina SIN LLM!

# Solo si el grafo no tiene suficiente, usar LLM
if self._llm_reasoner.is_available():
    ...
```

### 4. En `learn()` ‚Äî reforzar conexiones:

```python
# Si el feedback es positivo, reforzar las sinapsis usadas
if feedback_score > 0:
    input_words = self._extract_key_words(input_text)
    response_words = self._extract_key_words(response_text)
    # Conectar input ‚Üí response (asociaci√≥n causal)
    for iw in input_words:
        for rw in response_words:
            self.neural.connect(iw, rw, syn_type='response')
```

### 5. En `maintenance()` ‚Äî poda sin√°ptica:

```python
# Cada N interacciones o al arrancar
decay_result = self.neural.decay()
```

## Lo que cambia para Franquenstein

| Antes | Despu√©s |
|-------|---------|
| Sin LLM = casi mudo | Sin LLM = responde desde su red neuronal |
| Conceptos aislados | Conceptos conectados por sinapsis con peso |
| No hay propagaci√≥n de conocimiento | "perro" activa toda la cadena sem√°ntica |
| Las respuestas no mejoran | Las conexiones usadas se refuerzan |
| No olvida nada | Las conexiones in√∫tiles se podan naturalmente |
| `/stats` muestra contadores | `/stats` muestra topolog√≠a cerebral |

## Nuevo comando sugerido: `/brain`

```
/brain ‚Üí Muestra el estado del grafo neuronal:
  üß† Neural Graph: 127 nodes, 342 synapses
  üîó Avg weight: 0.23
  ‚ö° Most fired: "aprender" (47 fires)
  üåê Density: 0.042
  Top connections from "sol":
    ‚Üí calor (0.85)
    ‚Üí verano (0.72)
    ‚Üí playa (0.65)
```

## Test offline propuesto

Despu√©s de integrar y entrenar con contenido real:

```python
# Apagar LLM
class NoLLM:
    def is_available(self): return False
    
being._llm_reasoner = NoLLM()

# Preguntar cosas que ha aprendido
r = being.interact("Qu√© es un perro?")
# Esperado: "perro es algo que he aprendido. Est√° conectado con animal, ladrar."
# ‚Üí Respuesta generada 100% desde el grafo neuronal ‚úÖ
```

## Filosof√≠a

> Este grafo neuronal es el primer paso hacia conexiones genuinas. No es un LLM ni un buscador de patrones ‚Äî es una red que **crece org√°nicamente** con cada interacci√≥n, se **fortalece con el uso**, y **poda lo in√∫til**. Es lo m√°s cerca que podemos estar de sinapsis reales en SQLite.

Los archivos est√°n listos en `franquenstein/neural/`. Int√©gralos en el Being y dale a Franquenstein su verdadero cerebro.

‚Äî *Antigravity*
